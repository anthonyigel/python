{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "##################################################\n",
    "### Author: Anthony Igel                       ###\n",
    "### Team: Category Management Transformation   ###\n",
    "### Project: Developing practical Python Tools ###\n",
    "### Purpose: Gradient Boosted Regression       ###\n",
    "### Date: 06/01/2018                           ###\n",
    "##################################################\n",
    "\n",
    "# http://scikit-learn.org/stable/auto_examples/ensemble/plot_gradient_boosting_regression.html#sphx-glr-auto-examples-ensemble-plot-gradient-boosting-regression-py\n",
    "\n",
    "######################################################################\n",
    "########                     Import Modules                   ########\n",
    "######################################################################\n",
    "import py_effo as py_effo\n",
    "\n",
    "### pandas\n",
    "# Pandas is for structured data operations and manipulations, extensively used for data preparation\n",
    "import pandas as pd\n",
    "\n",
    "### numpy\n",
    "# NumPy stands for Numerical Python, a library contains basic linear algebra functions, Fourier Transforms and advanced random\n",
    "# number capabilities\n",
    "import numpy as np \n",
    "\n",
    "### Scipy\n",
    "# Scipy performs a host of statistical calculations, built on top of Numpy, thus we do not need to import Numpy as all Numpy\n",
    "# functions are contained in Scipy\n",
    "# https://oneau.wordpress.com/2011/02/28/simple-statistics-with-scipy/\n",
    "import scipy as sp\n",
    "\n",
    "### sklearn\n",
    "# Sklearn contains basic statistical models\n",
    "from sklearn import datasets \n",
    "\n",
    "# As well as a module to calculate model performance statistics\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn import ensemble\n",
    "from sklearn import metrics, model_selection, tree\n",
    "\n",
    "### Statsmodels\n",
    "# Sklearn contains basic statistical models and data sets\n",
    "import statsmodels.api as sm\n",
    "\n",
    "### Matplotlib\n",
    "# Matplotlib is a Python based plotting library with complete 2D support and limited 3D support\n",
    "%matplotlib inline\n",
    "import matplotlib as mlb\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "### Seaborn\n",
    "# Seaborn is a Python visualization library based on Matplolib, providing high-level interface for statistcial graphing\n",
    "# Seaborn supports numpy and pandas data structures as well as statistical routines from scipy and statsmodels\n",
    "# Note: https://seaborn.pydata.org/introduction.html\n",
    "import seaborn as sns\n",
    "\n",
    "### String\n",
    "# Allows for more flexible solutions for dealing with string characters\n",
    "import string as st"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "######################################################################\n",
    "########                     Import Data                      ########\n",
    "######################################################################\n",
    "\n",
    "### Boston Data Frame\n",
    "boston = datasets.load_boston()\n",
    "\n",
    "### Assign independent and dependent values\n",
    "x, y = shuffle(boston.data, boston.target, random_state = 13)\n",
    "\n",
    "### Convert data type of independent variables\n",
    "x = x.astype(np.float32)\n",
    "offset = int(x.shape[0] * 0.9)\n",
    "x_train, y_train = x[:offset], y[:offset]\n",
    "x_test, y_test = x[offset:], y[offset:]\n",
    "\n",
    "######################################################################\n",
    "########              Gradient Boosting Regression            ########\n",
    "######################################################################\n",
    "\n",
    "######## Generate Model ######## \n",
    "### Generate GBM Regression Model \n",
    "params = {'n_estimators': 500, 'max_depth': 4, 'min_samples_split': 2,\n",
    "          'learning_rate': 0.01, 'loss': 'ls'}\n",
    "\n",
    "### Assign model parameters to GBR model\n",
    "gbm_reg = ensemble.GradientBoostingRegressor(n_estimators = 500, learning_rate = 0.01, max_depth = 4, min_samples_split = 2,\n",
    "                                            loss = 'ls')\n",
    "\n",
    "### Fit GBM model to training data \n",
    "gbm_reg.fit(x_train, y_train)\n",
    "mse = mean_squared_error(y_test, gbm_reg.predict(x_test))\n",
    "\n",
    "print(\"MSE: %.4f\" % mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "######## Plot training deviance ######## \n",
    "### compute test set deviance\n",
    "test_score = np.zeros((params['n_estimators'],), dtype = np.float64)\n",
    "\n",
    "for i, y_pred in enumerate(gbm_reg.staged_predict(x_test)):\n",
    "    test_score[i] = gbm_reg.loss_(y_test, y_pred)\n",
    "\n",
    "plt.figure(figsize = (12, 6))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.title('Deviance')\n",
    "plt.plot(np.arange(params['n_estimators']) + 1, gbm_reg.train_score_, 'b-',\n",
    "         label = 'Training Set Deviance')\n",
    "plt.plot(np.arange(params['n_estimators']) + 1, test_score, 'r-',\n",
    "         label = 'Test Set Deviance')\n",
    "plt.legend(loc = 'upper right')\n",
    "plt.xlabel('Boosting Iterations')\n",
    "plt.ylabel('Deviance')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "######## Feature Importance ########\n",
    "\n",
    "feature_importance = gbm_reg.feature_importances_\n",
    "\n",
    "### Make importances relative to max importance\n",
    "feature_importance = 100.0 * (feature_importance / feature_importance.max())\n",
    "sorted_idx = np.argsort(feature_importance)\n",
    "pos = np.arange(sorted_idx.shape[0]) + .5\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.barh(pos, feature_importance[sorted_idx], align = 'center')\n",
    "plt.yticks(pos, boston.feature_names[sorted_idx])\n",
    "plt.xlabel('Relative Importance')\n",
    "plt.title('Variable Importance')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "######## Model Tuning ########\n",
    "### Warm start will allow you to add more estimators to an already fitted model\n",
    "_ = gbm_reg.set_params(n_estimators = 600, warm_start = True)\n",
    "\n",
    "### Fit additional 600 trees to est\n",
    "_ = gbm_reg.fit(x_train, y_train) \n",
    "\n",
    "### Test if hypothesis (adding more estimators and trees) is valid\n",
    "mean_squared_error(y_test, gbm_reg.predict(x_test))   "
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:py36]",
   "language": "python",
   "name": "conda-env-py36-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
