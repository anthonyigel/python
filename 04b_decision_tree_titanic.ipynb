{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "##################################################\n",
    "### Author: Anthony Igel                       ###\n",
    "### Team: Category Management Transformation   ###\n",
    "### Project: Developing practical Python Tools ###\n",
    "### Purpose: Decision Tree                     ###\n",
    "### Date: 05/24/2018                           ###\n",
    "##################################################\n",
    "\n",
    "# https://blog.socialcops.com/engineering/machine-learning-python/\n",
    "\n",
    "######################################################################\n",
    "########                     Import Modules                   ########\n",
    "######################################################################\n",
    "import py_effo as py_effo\n",
    "### pandas\n",
    "# Pandas is for structured data operations and manipulations, extensively used for data preparation,\n",
    "import pandas as pd\n",
    "\n",
    "### numpy\n",
    "# NumPy stands for Numerical Python, a library contains basic linear algebra functions, Fourier Transforms and advanced random\n",
    "# number capabilities\n",
    "import numpy as np\n",
    "\n",
    "### Scipy\n",
    "# Scipy performs a host of statistical calculations, built on top of Numpy, thus we do not need to import Numpy as all Numpy\n",
    "# functions are contained in Scipy\n",
    "# https://oneau.wordpress.com/2011/02/28/simple-statistics-with-scipy/\n",
    "import scipy as sp\n",
    "\n",
    "### sklearn\n",
    "# Sklearn contains basic statistical models\n",
    "from sklearn.datasets import load_iris\n",
    "# As well as a module to calculate model performance statistics\n",
    "from sklearn import tree, preprocessing, metrics, model_selection \n",
    "import sklearn.ensemble as ske\n",
    "\n",
    "# Sklearn contains basic statistical models\n",
    "import sklearn as sk\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "# As well as a module to calculate model performance statistics\n",
    "\n",
    "    \n",
    "### Statsmodels\n",
    "# Sklearn contains basic statistical models and data sets\n",
    "import statsmodels.api as sm\n",
    "\n",
    "### Matplotlib\n",
    "# Matplotlib is a Python based plotting library with complete 2D support and limited 3D support\n",
    "%matplotlib inline\n",
    "import matplotlib as mlb\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "### Seaborn\n",
    "# Seaborn is a Python visualization library based on Matplolib, providing high-level interface for statistcial graphing\n",
    "# Seaborn supports numpy and pandas data structures as well as statistical routines from scipy and statsmodels\n",
    "# Note: https://seaborn.pydata.org/introduction.html\n",
    "import seaborn as sns\n",
    "\n",
    "### String\n",
    "# Allows for more flexible solutions for dealing with string characters\n",
    "import string as st\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "######################################################################\n",
    "########                     Import Data                      ########\n",
    "######################################################################\n",
    "\n",
    "### Import Titanic data set\n",
    "titanic_df = pd.read_csv(\"//nfs/analysis/analysis/kroger/category_management_transformation/mini_hack_days/python/titanic3.csv\")\n",
    "\n",
    "### View top 10 records of the data frame\n",
    "titanic_df.head(3)\n",
    "\n",
    "### Variable Information\n",
    "# pclass: Passenger Class (1 = first; 2 = second; 3 = third)\n",
    "# survived: Survival (0 = no; 1 = yes)\n",
    "# name: Name of passenger\n",
    "# sex: Male or female\n",
    "# age: Age of passenger\n",
    "# sibsp: Number of siblings/spouses aboard\n",
    "# parch: Number of parents/children aboard\n",
    "# ticket: Ticket number\n",
    "# fare: Passenger fare\n",
    "# cabin: Cabin\n",
    "# embarked: Port of embarkment (C = Cherbourg; Q = Queenstown; S = Southampton)\n",
    "# boat: Lifeboat (if survived)\n",
    "# body: Body number (if did not survive and body was recovered)\n",
    "# home.dest: Home destination from titanic\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "######################################################################\n",
    "########                 Data Exploration                     ########\n",
    "######################################################################\n",
    "\n",
    "######## Core Data Frame Info ########\n",
    "### Let's determine how the data types look for this data frame\n",
    "print('Data Type Information')\n",
    "print(titanic_df.info())\n",
    "print()\n",
    "### View descriptive statistics about data set\n",
    "print('Descriptive Statistics of Titanic Data')\n",
    "print(titanic_df.describe())\n",
    "print()\n",
    "### View if any of our data is null\n",
    "print('Summary of Nulls in Titanic Data')\n",
    "print(titanic_df.isnull().sum())\n",
    "\n",
    "### Dimension of df\n",
    "print('Number of Records: ' + str(len(titanic_df)))\n",
    "print()\n",
    "### Lets view the overall chance for survival\n",
    "print('Average Survival Rate: '+ str((titanic_df['survived'].mean().round(4)) * 100) + '%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "######## Variable Group Stats ########\n",
    "### Lets view the general composition of each class\n",
    "class_grouping = titanic_df.groupby(['pclass']).mean()\n",
    "print('Class Composition')\n",
    "print(class_grouping)\n",
    "print()\n",
    "\n",
    "### Lets view the general composition of each class & sex combination\n",
    "class_sex_grouping = titanic_df.groupby(['pclass','sex']).mean()\n",
    "print('Class & Sex Composition')\n",
    "print(class_sex_grouping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "######################################################################\n",
    "########                  Data Preparation                    ########\n",
    "######################################################################\n",
    "\n",
    "######## Clean Feature Names ########\n",
    "### This will ensure that all column names are stripped of whitespace\n",
    "titanic_df.rename(columns = lambda x: x.strip(), inplace = True)\n",
    "\n",
    "### We can also adjust the case of our metrics table columns\n",
    "titanic_df.rename(columns = lambda x: x.lower(), inplace = True)\n",
    "\n",
    "######## Clean Null Values ########\n",
    "### Lets view the count of non-null values\n",
    "print('Count of Records Pre-Clean')\n",
    "print(titanic_df.count())\n",
    "print()\n",
    "### Most of the values in boat or cabin are missing, so we can delete these features\n",
    "titanic_df = titanic_df.drop(['body','cabin','boat'], axis=1)\n",
    "### Home destination isn't necessarily vaulable \n",
    "titanic_df[\"home.dest\"] = titanic_df[\"home.dest\"].fillna(\"NA\")\n",
    "titanic_df = titanic_df.dropna()\n",
    "print('Count of Records Post-Clean')\n",
    "print(titanic_df.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "######## Format Data for ML Processing ########\n",
    "### The preprocessing function from Sklearn will take our string value fields, \"sex\" & \"destination\", and convert to them\n",
    "### integer fields\n",
    "# sex: Female = 0; Male = 1\n",
    "# embarked: Cherbourg = 1; Queenstown = 2; Southampton = 3\n",
    "### The features \"name\", \"ticket\" and \"home.dest\" are not categorical, thus difficult to use in a classification algorithm\n",
    "### Therefore we will drop them\n",
    "def preprocess_titanic_df(df):\n",
    "    processed_df = df.copy()\n",
    "    le = preprocessing.LabelEncoder()\n",
    "    processed_df.sex = le.fit_transform(processed_df.sex)\n",
    "    processed_df.embarked = le.fit_transform(processed_df.embarked)\n",
    "    processed_df = processed_df.drop(['name','ticket','home.dest'],axis=1)\n",
    "    return processed_df\n",
    "\n",
    "processed_df = preprocess_titanic_df(titanic_df)\n",
    "\n",
    "print(processed_df.info())\n",
    "print()\n",
    "print(processed_df.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "######################################################################\n",
    "########                 Model Preparation                    ########\n",
    "######################################################################\n",
    "\n",
    "### Split the data set into dependent and independent variables\n",
    "x = processed_df.drop(['survived'], axis = 1).values\n",
    "y = processed_df['survived'].values\n",
    "\n",
    "### Split the data set into training and testing data sets; 80 - 20 split\n",
    "x_train, x_test, y_train, y_test = model_selection.train_test_split(x, y, test_size = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "######################################################################\n",
    "########                Decision Tree Modeling                ########\n",
    "######################################################################\n",
    "# http://scikit-learn.org/stable/modules/tree.html#tree\n",
    "\n",
    "### Initialize Decision Tree Object, an untrained Decision Tree Classifier with maximum tree depth set to 1-\n",
    "clf_dt = tree.DecisionTreeClassifier(max_depth = 10)\n",
    "\n",
    "### Fit the Decision Tree with the training data sets\n",
    "### This enables the Decision Tree model to \"learn\" how different features affect survivability\n",
    "clf_dt.fit (x_train, y_train)\n",
    "\n",
    "### Score the Decision Tree with the testing data sets\n",
    "print('% of Correctly Predicted Survivals: ' + str((clf_dt.score (x_test, y_test) * 100).round(2)) + '%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### Now if you are under the assumption that resulting scores could differ depending on which rows are selected for the training\n",
    "### and testing data sets, we can get around this by using a shuffle validator\n",
    "\n",
    "### The shuffle validator will apply the same 80-20 split as before, but generates 20 unique permutations of this split\n",
    "### By passing the shuffle validator as a parameter to the cross_val_score function we can score our classifer against each\n",
    "### of the different splits and compute their accuracy\n",
    "\n",
    "from sklearn import cross_validation\n",
    "\n",
    "shuffle_validator = cross_validation.ShuffleSplit(len(x), n_iter = 20, test_size = 0.2, train_size = 0.8, random_state = 0)\n",
    "def test_classifier(clf):\n",
    "    scores = cross_validation.cross_val_score(clf, x, y, cv = shuffle_validator)\n",
    "    print(\"Accuracy: %0.4f (+/- %0.2f)\" % (scores.mean(), scores.std()))\n",
    "\n",
    "### Thus, no matter how we split the data we will recieve this % of correctly predicted survivals and the following \n",
    "### standard deviation \n",
    "test_classifier(clf_dt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### Create Vector of Predicted values\n",
    "pred_dt = clf_dt.predict(x)\n",
    "pred_dt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### Additionally, we can use scikit-learn to easily test other machine learning algorithms using the same syntax\n",
    "clf_rf = ske.RandomForestClassifier(n_estimators=50)\n",
    "print('Random Forest Classification')\n",
    "test_classifier(clf_rf)\n",
    "print()\n",
    "print('Gradient Boosting Classifier')\n",
    "clf_gb = ske.GradientBoostingClassifier(n_estimators=50)\n",
    "test_classifier(clf_gb)\n",
    "print()\n",
    "print('Voting Classifier')\n",
    "eclf = ske.VotingClassifier([('dt', clf_dt), ('rf', clf_rf), ('gb', clf_gb)])\n",
    "test_classifier(eclf)\n",
    "\n",
    "######## Random Forest Classification ########\n",
    "### The Random Forest algorithm will create a multitude of tree for the data set, generally very poor, using different random\n",
    "## subsets of the input variables. Then returns whichever prediction was returned by the most trees to avoid \"overfitting\"\n",
    "\n",
    "### Overfitting occurs when a model is very tightly fitted to arbitrary correlations in the training data that it performs\n",
    "## poorly on the testing data \n",
    "\n",
    "######## Gradient Boosting Classification ########\n",
    "### The Gradient Boosting algorithm will again, generate many weak/shallow trees, and then combine (boost) them into a strong\n",
    "## model\n",
    "## GBM performs well on our df, but is often relatively slow and difficult to optimize since the model construction happens\n",
    "## sequentially and cannot be performed in parallel\n",
    "\n",
    "######## Voting Classification ########\n",
    "### The Voting algorithm can be used to apply multiple conceptually divergent classification models to the same data set and\n",
    "## will return the majority vote from all of the classifiers\n",
    "## E.g. if the GBM predicts that a passenger will not survive, but the Decision Tree and Random Forest both predict that the \n",
    "## same passenger will survive, the Voting classifier will chose the latter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### Display level of Importance for each Feature\n",
    "clf_dt.feature_importances_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### Get information on parameters used in Decision Tree Model\n",
    "clf_dt.get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### Get information on parameters used in Random Forest Model\n",
    "clf_rf.get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "######################################################################\n",
    "########                Logisitc Modeling                     ########\n",
    "######################################################################\n",
    "\n",
    "######## Model Preparation ######## \n",
    "logreg = pd.DataFrame(processed_df)\n",
    "\n",
    "### Split the data set into dependent and independent variables\n",
    "x2 = logreg.drop(['survived'], axis = 1).values\n",
    "y2 = logreg['survived'].values\n",
    "\n",
    "### Split the data set into training and testing data sets; 80 - 20 split\n",
    "x2_train, x2_test, y2_train, y2_test = model_selection.train_test_split(x2, y2, test_size = 0.2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "######## Modeling  With Full Data ########\n",
    "\n",
    "### Lets run a logistic regression on the entire data set and see if it accurate\n",
    "# instantiate a logistic regression model, and fit with X and y\n",
    "model = LogisticRegression()\n",
    "model = model.fit(x2, y2)\n",
    "\n",
    "# check the accuracy on the training set\n",
    "ModelScore_1 = model.score(x2, y2)\n",
    "print('Model Score: ' + str((ModelScore_1 * 100).round(0)) + '%')\n",
    "print()\n",
    "# what percentage had affairs?\n",
    "PredMean_1 = y2.mean()\n",
    "print('What % of Passengers Survived? '+ str((PredMean_1 * 100).round(0)) + '%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "######## Modeling  With Train/Test Data ########\n",
    "\n",
    "model2 = LogisticRegression()\n",
    "model2.fit(x2_train, y2_train)\n",
    "\n",
    "### Predict class labels for the test set\n",
    "predicted = model2.predict(x2_test)\n",
    "\n",
    "print('Predicted Values')\n",
    "print(predicted)\n",
    "print()\n",
    "### Generate class probabilities\n",
    "probs = model2.predict_proba(x2_test)\n",
    "print('Prediction Probabilities')\n",
    "print(probs)\n",
    "print()\n",
    "### Generate evaluation metrics\n",
    "AccuracyScore_2 = metrics.accuracy_score(y2_test, predicted)\n",
    "RocAucScore_2 = metrics.roc_auc_score(y2_test, probs[:, 1])\n",
    "print('Accuracy Score: ' + str((AccuracyScore_2 * 100).round(0)) + '%')\n",
    "print('ROC AUC Score: ' + str((RocAucScore_2 * 100).round(0)) + '%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "######## Additional Reports ########\n",
    "### Confusion Matrix\n",
    "## A confusion matrix is a summary of prediction results on a classification problem\n",
    "## The number of correct and incorrect predictions are summarised with count values\n",
    "\n",
    "## true prediction | false prediction\n",
    "## false prediction | true prediction\n",
    "print('Confusion Matrix')\n",
    "print(metrics.confusion_matrix(y2_test, predicted))\n",
    "print()\n",
    "### Classification Report\n",
    "print('Classification Report')\n",
    "print(metrics.classification_report(y2_test, predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "######################################################################\n",
    "########                  Model Validation                    ########\n",
    "######################################################################\n",
    "\n",
    "### Evaluate the model using 10-fold cross-validation\n",
    "scores = cross_val_score(LogisticRegression(), x2, y2, scoring = 'accuracy', cv = 10)\n",
    "print('Cross Validation Scores')\n",
    "print((scores * 100).round(0))\n",
    "print()\n",
    "print('Average Scores')\n",
    "print(str((scores.mean() * 100).round(0)) + '%')\n",
    "\n",
    "### Model is performing with 77% accuracy"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:py36]",
   "language": "python",
   "name": "conda-env-py36-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
